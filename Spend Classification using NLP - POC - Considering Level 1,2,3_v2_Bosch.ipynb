{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sundararaman\n",
      "[nltk_data]     Sairam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sundararaman\n",
      "[nltk_data]     Sairam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sundararaman Sairam\n"
     ]
    }
   ],
   "source": [
    "# Importing required packages\n",
    "import time\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "program_start_time  = time.process_time()\n",
    "import os\n",
    "from autocorrect import Speller\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "stemmer = PorterStemmer()\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from pprint import pprint\n",
    "nltk.download('wordnet')\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Allowing notebook to display all rows and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Import \n",
    "#original \n",
    "path=os.getcwd()\n",
    "input_df=pd.read_csv(\"C:/Work/NLP_POC/01.Input Data/Bosch/Latest_Mapped_Data_20181013.csv\",encoding='Latin-1')\n",
    "filtered_df=input_df[[\"Primary Key\",\"Validation status\",\"features_preproc\",\"Translate\"]]\n",
    "del(input_df)\n",
    "\n",
    "# Removing records with blank Primary key values and only records in validation status that are validated by BCG\n",
    "filtered_df=filtered_df[~(filtered_df['Primary Key'].isna()) ]\n",
    "#filtered_df=filtered_df[(filtered_df['Validation status']==\"Validated by BCG\")]\n",
    "filtered_df=filtered_df[(filtered_df['Validation status']==\"Validated by BCG\") | (filtered_df['Validation status']== \"Validated by BCG - To be checked\") | (filtered_df['Validation status']== \"Single BCG L3 mapped - Pending QC\") | (filtered_df['Validation status']== \" Single BCG L3 Mapped - Pending QC\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inner Join with mapping file get correspoding labels\n",
    "#Inner Join with mapping file get correspoding labels\n",
    "filtered_df['Level1']=filtered_df['Primary Key'].apply(lambda x:(x.split('|'))[0])\n",
    "filtered_df['Level2']=filtered_df['Primary Key'].apply(lambda x:(x.split('|'))[1])\n",
    "filtered_df['Level3']=filtered_df['Primary Key'].apply(lambda x:(x.split('|'))[2])\n",
    "filtered_df['Level1']=filtered_df['Level1'].apply(lambda x : (x.strip()).upper())\n",
    "filtered_df['Level2']=filtered_df['Level2'].apply(lambda x : (x.strip()).upper())\n",
    "filtered_df['Level3']=filtered_df['Level3'].apply(lambda x : (x.strip()).upper())\n",
    "filtered_df['Primary Key_new']=filtered_df['Level1']+'|'+ filtered_df['Level2'] + '|' +  filtered_df['Level3']\n",
    "#Please look at the data once to ensure correct concatenation\n",
    "\n",
    "#Dropping unnecessary columns\n",
    "filtered_df=filtered_df.drop(['Primary Key','Validation status','Level1','Level2','Level3','Translate'],axis=1)\n",
    "#Renaming the column of primary key as Level1 to maintain nomenclature\n",
    "filtered_df.rename(columns={'Primary Key_new':'Level1'},inplace=True)\n",
    "filtered_df['originalInvoiceText']=filtered_df['features_preproc']\n",
    "filtered_df['Invoice Text']=filtered_df['originalInvoiceText']\n",
    "filtered_df['data_source']='Bosch'\n",
    "filtered_df=filtered_df.drop(['features_preproc'],axis=1)\n",
    "\n",
    "# # Removing keyword translate and [] brackets from invoice text\n",
    "filtered_df['Invoice Text'] = filtered_df['Invoice Text'].str.replace(\"\\[Translation: \",\"\")\n",
    "filtered_df['Invoice Text'] = filtered_df['Invoice Text'].str.replace(\"]\",\"\")\n",
    "full_df=filtered_df[['Invoice Text', 'Level1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create sgement percentage dataset for input column\n",
    "def column_creation(column_name,input_df):\t\n",
    "\tdataset_name=pd.DataFrame()\n",
    "\tdataset_name=input_df[column_name].value_counts().rename_axis(column_name).reset_index(name='counts')\n",
    "\tdataset_name['seg_perc']=(dataset_name['counts']/(dataset_name['counts'].sum()))*100\n",
    "\n",
    "\treturn(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(823735, 2)\n",
      "(61780, 2)\n"
     ]
    }
   ],
   "source": [
    "df=full_df.drop_duplicates(inplace = False)\n",
    "print(full_df.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique classes are- 20\n"
     ]
    }
   ],
   "source": [
    "#Removing Duplicate records\n",
    "df=full_df.drop_duplicates(inplace = False)\n",
    "\n",
    "#df = full_df.copy()\n",
    "df['originalInvoiceText'] = df[[\"Invoice Text\"]].astype(str)\n",
    "df[[\"Invoice Text\"]] = df[[\"Invoice Text\"]].astype(str)\n",
    "\n",
    "#Creating summary statistics, %Level to identify and eiliminate low percentage \n",
    "summary=column_creation('Level1',df)\n",
    "\n",
    "#Joining and getting the summary statistics\n",
    "df = pd.merge(df,summary, how = 'left', left_on = ['Level1']\n",
    "                       , right_on = ['Level1'])[['Level1','originalInvoiceText','Invoice Text','seg_perc']]\n",
    "\n",
    "#Percentage threshold\n",
    "threshold=0.5\n",
    "df=df[df['seg_perc']>threshold]\n",
    "\n",
    "# Creating index which will come into use later while joining\n",
    "df['index'] = df.reset_index().index\n",
    "\n",
    "# Reordering data\n",
    "#df = df[['index','Level1','originalInvoiceText','Invoice Text']]\n",
    "\n",
    "#Unique classes check\n",
    "print(\"unique classes are-\",df['Level1'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Data Pre-Processing (Basic) starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember - the original raw text is retained in the column originalInvoiceText\n",
    "\n",
    "#Step 1 - Uniform Case Conversion\n",
    "# Converting text to lower case\n",
    "df[\"Invoice Text\"] = df[\"Invoice Text\"].str.lower()\n",
    "\n",
    "#Step 2 Digits Removal\n",
    "# Removing digits from text since SKIP GRAM can't handle digits and the digits are in the form of dates  -20170908 and dont add any value\n",
    "def remove_digits(text):\n",
    "    return \"\".join([i for i in text if not i.isdigit()])\n",
    "\n",
    "df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:remove_digits(text))\n",
    "\n",
    "#Step 3 Replace Punctuations and special characters with space so that work demaraction is maintained\n",
    "# Removing punctuations\n",
    "PUNCT_TO_REMOVE =string.punctuation\n",
    "def rem_punctuation(text):\n",
    "    \n",
    "    \"\"\"custom function to remove the punctuation\"\"\"  \n",
    "  \n",
    "    return  re.sub(r\"[!%:*<>=/(){}^'~|\\_,.;@#?!&$Â¡Â¢Â£Â¤Â¥Â¦+Â§Â¨Â©ÂªÂ«Â¬Â®Â¯Â°Â±Â²Â³Â´ÂµÂ¶Â·Â¸Â¹ÂºÂ»Â¼Â½Â¾Â¿Ã Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã—Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾ÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã·Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾Ã¿]+\\ *\", \" \", text)  \n",
    "df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:rem_punctuation(text))\n",
    "df[\"Invoice Text\"] = df[\"Invoice Text\"].str.replace(\"-\",\" \")\n",
    "\n",
    "#Step 4  - Remove Stopwords (Enhanced list (jan,january etc )\n",
    "# Removal of stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "#Adding the most commonly used relevant stopwords\n",
    "STOPWORDS=STOPWORDS|{\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "                    \"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",    \n",
    "                    \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\", \"cannot\", \"cant\", \"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"each\", \"eg\", \"either\", \"else\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\", \"four\", \"from\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"she\", \"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n",
    "                    \"this\", \"those\", \"though\", \"through\", \"throughout\",\n",
    "                    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n",
    "                    \"under\", \"until\", \"up\", \"upon\", \"us\",\"aaacr\",\"aajit\",\"stupid\",\"ass\",\"nan\",\n",
    "                    \"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "                    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "                    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \n",
    "                    \"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "                    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "                     'jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','january','february','march','april','may','june','july','august','september','october','november','december',\n",
    "\n",
    "                    }\n",
    "def rem_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:rem_stopwords(text))\n",
    "\n",
    "#Step 5 - Lemmatization\n",
    "# Lemmatization\n",
    "# Lemmatization - Converting all variants of the words to their 'verb' form\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word,\"v\") for word in text.split()])\n",
    "df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:lemmatize_words(text))\n",
    "\n",
    "#Step 6 - Remove short words\n",
    "#Function to remove words with length<=2\n",
    "def short_words(text):\n",
    "    return \" \".join([word for word in str(text).split() if len(word)>3])\n",
    "df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:short_words(text))\n",
    "\n",
    "#Step 7 Rare words removal\n",
    "\n",
    "#Word frequency count after data pre-processing\n",
    "# Removal of frequent words\n",
    "cnt = Counter()\n",
    "for text in df[\"Invoice Text\"].values:\n",
    "    for word in str(text).split():\n",
    "        cnt[word]+=1\n",
    "        \n",
    "cnt.most_common(10)\n",
    "# The most common words seem to be quite indicative of the classes. Hence retaining them for now\n",
    "\n",
    "# Removal of rare words\n",
    "n_rare_words = 100\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "\n",
    "# The most rare words seem to be junk values and not add any predictive value to the model. Hence removing them from the corpus\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:remove_rarewords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the pre-processed data out as a csv file after removing blank texts\n",
    "df_preprocessed = df.copy()\n",
    "\n",
    "# Removing text with only 1 word\n",
    "\n",
    "def remove_shortwords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "df_preprocessed[\"length_row\"] = df_preprocessed[\"Invoice Text\"].apply(lambda text:remove_shortwords(text))\n",
    "df_preprocessed = df_preprocessed.loc[df_preprocessed['length_row']>2,['index','Level1','Invoice Text']]\n",
    "\n",
    "#df_preprocessed.to_csv(\"C:/Work/NLP_POC/03.Output Data/DFI/Considering only level 1/Prediction Results/Removing 4 most sparse labels 0.5% threshold\\preProcessedDataWithoutSparseLabels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Splitting into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the corpus is (38936, 2)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "modelingDataset = df_preprocessed[['Level1','Invoice Text']].copy()\n",
    "modelingDataset.reset_index(drop=True)\n",
    "\n",
    "#Percentage threshold\n",
    "summary=column_creation('Level1',modelingDataset)\n",
    "\n",
    "#Joining and getting the summary statistics\n",
    "modelingDataset = pd.merge(modelingDataset,summary, how = 'left', left_on = ['Level1']\n",
    "                       , right_on = ['Level1'])\n",
    "threshold=0.5\n",
    "modelingDataset=modelingDataset[modelingDataset['seg_perc']>threshold]\n",
    "\n",
    "X = modelingDataset['Invoice Text']\n",
    "y = modelingDataset['Level1']\n",
    "\n",
    "modelingDataset = modelingDataset[['Level1','Invoice Text']]\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(X,y,test_size=0.30,random_state=120)\n",
    "\n",
    "# Creating copy of datasets\n",
    "train_x_copy = pd.DataFrame(train_x.copy())\n",
    "valid_x_copy = pd.DataFrame(valid_x.copy())\n",
    "train_y_copy = pd.DataFrame(train_y.copy())\n",
    "valid_y_copy = pd.DataFrame(valid_y.copy())\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "#Corpus size\n",
    "print(\"Size of the corpus is\",modelingDataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelingDataset.to_csv(\"C:/Work/NLP_POC/03.Output Data/DFI/Considering only level 1/Prediction Results/Removing 4 most sparse labels 0.5% threshold\\modelingDataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Creation of n gram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,2))\n",
    "tfidf_vect_ngram.fit(modelingDataset['Invoice Text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Performing Modeling iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a generic funtion that can be invoked to run different model iterations\n",
    "# It accepts the classifier, feature_vector of training data, labels and feature vectors of valid data as inputs\n",
    "# Using these inputs, the model is trained and accuracy score is computed\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,is_neural_net=False):\n",
    "\n",
    "    # fit the training dataset on the classifier\n",
    "    clf = classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = clf.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "        \n",
    "    return clf,predictions,(metrics.accuracy_score(predictions, valid_y)*100),(metrics.balanced_accuracy_score(predictions, valid_y)*100),(metrics.f1_score(predictions, valid_y,average='macro')*100),(metrics.f1_score(predictions, valid_y,average='weighted')*100),(metrics.precision_score(predictions, valid_y,average='macro')*100),(metrics.precision_score(predictions, valid_y,average='weighted')*100),(metrics.recall_score(predictions, valid_y,average='macro')*100),(metrics.recall_score(predictions, valid_y,average='weighted')*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes on Word Level TF IDF Vectors\n",
    "# # Balanced accuracy is average of recall per class\n",
    "# start = time.process_time()\n",
    "# clf, predictions,accuracy,balanced_accuracy,macro_f1,weighted_f1,macro_precision,weighted_precision,macro_recall,weighted_recall  = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "# print (\"NB, WordLevel TF-IDF: Accuracy: \", round(accuracy,2))\n",
    "# print (\"NB, WordLevel TF-IDF: Balanced Accuracy: \", round(balanced_accuracy,2))\n",
    "# print (\"NB, WordLevel TF-IDF: Macro F1 score: \", round(macro_f1,2))\n",
    "# print (\"NB, WordLevel TF-IDF: Weighted F1 score: \", round(weighted_f1,2))\n",
    "# print (\"NB, WordLevel TF-IDF: Macro Precision score: \", round(macro_precision,2))\n",
    "# print (\"NB, WordLevel TF-IDF: Weighted Precision score: \", round(weighted_precision,2))\n",
    "# print (\"NB, WordLevel TF-IDF: Macro Recall score: \", round(macro_recall,2))\n",
    "# print (\"NB, WordLevel TF-IDF: Weighted Recall score: \", round(weighted_recall,2))\n",
    "\n",
    "# end = time.process_time()\n",
    "# print(\"Time taken to run NB model:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Comparing NB Preditions vs Actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = encoder.inverse_transform(predictions)\n",
    "# predictions = pd.DataFrame(predictions)\n",
    "# predictions.rename(columns={0:'Predicted Class'},inplace=True)\n",
    "# predictions = pd.concat([predictions.reset_index(drop=True), valid_y_copy.reset_index(drop=True),valid_x_copy.reset_index(drop=True)], axis=1)\n",
    "# predictions.to_csv(\"C:/Work/NLP_POC/03.Output Data/DFI/Considering only level 1/Prediction Results/Removing 4 most sparse labels 0.5% threshold/predictionResultsWithoutSparseLabels_4MultinominalNBModel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Suport Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF: Accuracy:  88.97\n",
      "SVM, WordLevel TF-IDF: Balanced Accuracy:  80.48\n",
      "SVM, WordLevel TF-IDF: Macro F1 score:  77.36\n",
      "SVM, WordLevel TF-IDF: Weighted F1 score:  89.33\n",
      "SVM, WordLevel TF-IDF: Macro Precision score:  75.3\n",
      "SVM, WordLevel TF-IDF: Weighted Precision score:  89.95\n",
      "SVM, WordLevel TF-IDF: Macro Recall score:  80.48\n",
      "SVM, WordLevel TF-IDF: Weighted Recall score:  88.97\n",
      "Time taken to run SVM model: 0.890625\n"
     ]
    }
   ],
   "source": [
    "# SVM on Word Level TF IDF Vectors\n",
    "# Balanced accuracy is average of recall per class\n",
    "from sklearn.svm import LinearSVC\n",
    "start = time.process_time()\n",
    "clf, predictions,accuracy,balanced_accuracy,macro_f1,weighted_f1,macro_precision,weighted_precision,macro_recall,weighted_recall  = train_model(LinearSVC(random_state=0,tol=1e-5), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, WordLevel TF-IDF: Accuracy: \", round(accuracy,2))\n",
    "print (\"SVM, WordLevel TF-IDF: Balanced Accuracy: \", round(balanced_accuracy,2))\n",
    "print (\"SVM, WordLevel TF-IDF: Macro F1 score: \", round(macro_f1,2))\n",
    "print (\"SVM, WordLevel TF-IDF: Weighted F1 score: \", round(weighted_f1,2))\n",
    "print (\"SVM, WordLevel TF-IDF: Macro Precision score: \", round(macro_precision,2))\n",
    "print (\"SVM, WordLevel TF-IDF: Weighted Precision score: \", round(weighted_precision,2))\n",
    "print (\"SVM, WordLevel TF-IDF: Macro Recall score: \", round(macro_recall,2))\n",
    "print (\"SVM, WordLevel TF-IDF: Weighted Recall score: \", round(weighted_recall,2))\n",
    "\n",
    "end = time.process_time()\n",
    "print(\"Time taken to run SVM model:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Class-wise confusion matrix\n",
    "# from sklearn.metrics import classification_report\n",
    "# valid_y = encoder.inverse_transform(valid_y)\n",
    "# predictions = encoder.inverse_transform(predictions)\n",
    "# target_names = np.asarray(train_y_copy['Level1'].unique())\n",
    "# print(classification_report(valid_y,predictions,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = pd.DataFrame(predictions)\n",
    "# predictions.rename(columns={0:'Predicted Class'},inplace=True)\n",
    "# predictions = pd.concat([predictions.reset_index(drop=True), valid_y_copy.reset_index(drop=True),valid_x_copy.reset_index(drop=True)], axis=1)\n",
    "# predictions.to_csv(\"C:/Work/NLP_POC/03.Output Data/DFI/Considering only level 1/Prediction Results/Removing 4 most sparse labels 0.5% threshold/predictionResultsWithoutSparseLabels_4_SVMModel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RF on Word Level TF IDF Vectors\n",
    "# # Balanced accuracy is average of recall per class\n",
    "# start = time.process_time()\n",
    "# clf, predictions,accuracy,balanced_accuracy,macro_f1,weighted_f1,macro_precision,weighted_precision,macro_recall,weighted_recall  = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "# print (\"RF, WordLevel TF-IDF: Accuracy: \", round(accuracy,2))\n",
    "# print (\"RF, WordLevel TF-IDF: Balanced Accuracy: \", round(balanced_accuracy,2))\n",
    "# print (\"RF, WordLevel TF-IDF: Macro F1 score: \", round(macro_f1,2))\n",
    "# print (\"RF, WordLevel TF-IDF: Weighted F1 score: \", round(weighted_f1,2))\n",
    "# print (\"RF, WordLevel TF-IDF: Macro Precision score: \", round(macro_precision,2))\n",
    "# print (\"RF, WordLevel TF-IDF: Weighted Precision score: \", round(weighted_precision,2))\n",
    "# print (\"RF, WordLevel TF-IDF: Macro Recall score: \", round(macro_recall,2))\n",
    "# print (\"RF, WordLevel TF-IDF: Weighted Recall score: \", round(weighted_recall,2))\n",
    "\n",
    "# end = time.process_time()\n",
    "# print(\"Time taken to run RF model:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total module run time: 26.765625\n"
     ]
    }
   ],
   "source": [
    "program_end_time = time.process_time()\n",
    "print(\"Total module run time:\", program_end_time-program_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous + Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of SKIP-GRAM word embedding file starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the pre-trained word-embedding vectors \n",
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# import pandas, xgboost, numpy, textblob, string\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers\n",
    "\n",
    "# embeddings_index = {}\n",
    "# for i, line in enumerate(open('C:/Work/NLP_POC/01.Input Data/DFI/wiki-news-300d-1M.vec')):\n",
    "#     values = line.split()\n",
    "#     embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# # create a tokenizer \n",
    "# token = text.Tokenizer()\n",
    "# token.fit_on_texts(modelingDataset['Invoice Text'])\n",
    "# word_index = token.word_index\n",
    "\n",
    "# # split the dataset into training and validation datasets \n",
    "# modelingDataset = df_preprocessed[['Level1','Invoice Text']].copy()\n",
    "# modelingDataset.reset_index(drop=True)\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(modelingDataset['Invoice Text'], modelingDataset['Level1'],test_size=0.30,random_state=100)\n",
    "\n",
    "# # Creating copy of datasets\n",
    "# train_x_copy = pd.DataFrame(train_x.copy())\n",
    "# valid_x_copy = pd.DataFrame(valid_x.copy())\n",
    "# train_y_copy = pd.DataFrame(train_y.copy())\n",
    "# valid_y_copy = pd.DataFrame(valid_y.copy())\n",
    "\n",
    "# # label encode the target variable \n",
    "# encoder = preprocessing.LabelEncoder()\n",
    "# train_y = encoder.fit_transform(train_y)\n",
    "# valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "# # convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "# train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=100)\n",
    "# valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=100)\n",
    "\n",
    "# # create token-embedding mapping\n",
    "# embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# import pandas, xgboost, numpy, textblob, string\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Doing cross validation\n",
    "\n",
    "# from sklearn.model_selection import KFold,cross_val_score\n",
    "# kfold = KFold(n_splits=10, random_state=101)\n",
    "\n",
    "# def cv_model(classifier, feature_vector_train, label, feature_vector_valid,kfold,is_neural_net=False):\n",
    "\n",
    "#     # fit the training dataset on the classifier\n",
    "#     #classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "#     print(cross_val_score(classifier, feature_vector_train, label, cv=kfold))\n",
    "    \n",
    "#     # predict the labels on validation dataset\n",
    "#     #predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "#     #return predictions,(metrics.accuracy_score(predictions, valid_y)*100),(metrics.balanced_accuracy_score(predictions, valid_y)*100),(metrics.f1_score(predictions, valid_y,average='macro')*100),(metrics.f1_score(predictions, valid_y,average='weighted')*100),(metrics.precision_score(predictions, valid_y,average='macro')*100),(metrics.precision_score(predictions, valid_y,average='weighted')*100),(metrics.recall_score(predictions, valid_y,average='macro')*100),(metrics.recall_score(predictions, valid_y,average='weighted')*100)\n",
    "# cv_model(LinearSVC(random_state=0,tol=1e-5), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram,kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All obsolete codes below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell correction code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Running spellcheck using Parallel processing\n",
    "# # Not including this as part of the code right now, because processing time is huge\n",
    "\n",
    "# import threading\n",
    "# results = []\n",
    "# from spellchecker import SpellChecker\n",
    "# spell = SpellChecker()\n",
    "# def correct_spellings(text):\n",
    "#     corrected_text = []\n",
    "#     misspelled_words = spell.unknown(text.split())\n",
    "#     for word in text.split():\n",
    "#         if word in misspelled_words:\n",
    "#             corrected_text.append(spell.correction(word))\n",
    "#         else:\n",
    "#             corrected_text.append(word)\n",
    "#     return \" \".join(corrected_text)\n",
    "\n",
    "# from multiprocessing.dummy import Pool as ThreadPool\n",
    "# pool = ThreadPool(4)\n",
    "# results = pool.map(correct_spellings, df[\"Invoice Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from spellchecker import SpellChecker\n",
    "# # spell = SpellChecker()\n",
    "# # def correct_spellings(text):\n",
    "# #     corrected_text = []\n",
    "# #     misspelled_words = spell.unknown(text.split())\n",
    "# #     for word in text.split():\n",
    "# #         if word in misspelled_words:\n",
    "# #             corrected_text.append(spell.correction(word))\n",
    "# #         else:\n",
    "# #             corrected_text.append(word)\n",
    "# #     return \" \".join(corrected_text)\n",
    "# # from autocorrect import Speller\n",
    "# # df[\"Invoice Text\"] = df[\"Invoice Text\"].astype(str)\n",
    "# # #df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda x: \" \".join([Speller(i) for i in x.split()]))\n",
    "# # df[\"Invoice Text\"] = [' '.join([Speller(i) for i in x.split()]) for x in df[\"Invoice Text\"]]\n",
    "\n",
    "# star_time = time.process_time()\n",
    "# from autocorrect import spell\n",
    "# import concurrent.futures\n",
    "# import os\n",
    "# print(os.cpu_count())\n",
    "# from multiprocessing.dummy import Pool as ThreadPool\n",
    "# pool = ThreadPool(4)\n",
    "\n",
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# spell = SpellChecker()\n",
    "# def correct_spellings(text):\n",
    "#     corrected_text = []\n",
    "#     misspelled_words = spell.unknown(text.split())\n",
    "#     for word in text.split():\n",
    "#         if word in misspelled_words:\n",
    "#             corrected_text.append(spell.correction(word))\n",
    "#         else:\n",
    "#             corrected_text.append(word)\n",
    "#     return \" \".join(corrected_text)\n",
    "\n",
    "# result = pool.map(correct_spellings, df[\"Invoice Text\"])\n",
    "\n",
    "# #df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:correct_spellings(text))\n",
    "\n",
    "# end_time = time.process_time()\n",
    "\n",
    "# print(\"Time taken to run auto correct module:\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing (Advanced) starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Note: Emoji, emoticons, HTML Tags, URLs may not be present in our data. This step is only a good to have\n",
    "# # # Removing emojis from text\n",
    "# def remove_emoji(string):\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                            u\"\\U00002702-\\U000027B0\"\n",
    "#                            u\"\\U000024C2-\\U0001F251\"\n",
    "#                            \"]+\", flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'', str(string))\n",
    "\n",
    "# df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:remove_emoji(text))\n",
    "\n",
    "# # Removal of emoticons\n",
    "# # :-) is an emoticon\n",
    "# # ðŸ˜€ is an emoji\n",
    "\n",
    "# #Importing libraries\n",
    "# import re\n",
    "# from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "\n",
    "# def remove_emoticons(text):\n",
    "#     emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "#     return emoticon_pattern.sub(r'', str(text))\n",
    "\n",
    "# remove_emoticons(\"Hello :-)\")\n",
    "\n",
    "# df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:remove_emoticons(text))\n",
    "\n",
    "# # Removal of URLs\n",
    "# def remove_urls(text):\n",
    "#     url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "#     return url_pattern.sub(r'', str(text))\n",
    "\n",
    "# df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:remove_urls(text))\n",
    "\n",
    "# # Removal of HTML Tags\n",
    "# from bs4 import BeautifulSoup\n",
    "# def remove_html(text):\n",
    "#     return BeautifulSoup(text, \"lxml\").text\n",
    "\n",
    "# df[\"Invoice Text\"] = df[\"Invoice Text\"].apply(lambda text:remove_html(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
